{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f13be5a",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - D·ª± √°n S·ª©c kh·ªèe RƒÉng mi·ªáng BRFSS 2022\n",
    "\n",
    "## M·ª•c ti√™u\n",
    "X√¢y d·ª±ng v√† ƒë√°nh gi√° 10 baseline models theo k·∫ø ho·∫°ch team:\n",
    "1. **Classical Models**: SVM, KNN\n",
    "2. **Boosting Models**: XGBoost, CatBoost, AdaBoost\n",
    "3. **Neural Networks**: MLP, 1D CNN, TabNet\n",
    "4. **Hyperparameter Tuning**: S·ª≠ d·ª•ng Optuna\n",
    "5. **Focus on Recall**: T·ªëi ∆∞u cho b√†i to√°n screening\n",
    "6. **XAI Analysis**: 5 lo·∫°i SHAP + LIME\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b266e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Import th√†nh c√¥ng t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt!\n",
      "üìÖ Th·ªùi gian b·∫Øt ƒë·∫ßu: 2025-07-06 16:56:50\n"
     ]
    }
   ],
   "source": [
    "# Import t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Classical Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Boosting Models\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Neural Networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# C·∫•u h√¨nh\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Import th√†nh c√¥ng t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt!\")\n",
    "print(f\"üìÖ Th·ªùi gian b·∫Øt ƒë·∫ßu: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6b070",
   "metadata": {},
   "source": [
    "## 1. T·∫£i Dataset ƒë√£ Clean v√† Chu·∫©n b·ªã Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cd46fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä T·∫¢I DATASET ƒê√É CLEAN\n",
      "========================================\n",
      "‚úÖ T·∫£i th√†nh c√¥ng dataset ƒë√£ clean\n",
      "üìä K√≠ch th∆∞·ªõc: 445,132 h√†ng √ó 115 c·ªôt\n",
      "üíæ B·ªô nh·ªõ: 505.59 MB\n",
      "‚ùì Missing values: 963,258\n",
      "\n",
      "üìã C√ÅC C·ªòT TRONG DATASET:\n",
      "   ['_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU', 'SEXVAR', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'PRIMINSR', 'PERSDOC3', 'MEDCOST1', 'CHECKUP1', 'EXERANY2', 'SLEPTIM1', 'LASTDEN4', 'RMVTETH4', 'CVDINFR4', 'CVDCRHD4', 'CVDSTRK3', 'ASTHMA3', 'CHCSCNC1', 'CHCOCNC1', 'CHCCOPD3', 'ADDEPEV3', 'CHCKDNY2', 'HAVARTH4', 'DIABETE4', 'MARITAL', 'EDUCA', 'RENTHOM1', 'CPDEMO1C', 'VETERAN3', 'EMPLOY1', 'CHILDREN', 'INCOME3', 'WEIGHT2', 'HEIGHT3', 'DEAF', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'USENOW3', 'ECIGNOW2', 'LCSCTSC1', 'ALCDAY4', 'FLUSHOT7', 'PNEUVAC4', 'TETANUS1', 'HIVTST7', 'HIVRISK5', 'COVIDPOS', 'QSTVER', 'QSTLANG', '_METSTAT', '_URBSTAT', '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_IMPRACE', '_DUALUSE', '_LLCPWT2', '_LLCPWT', '_RFHLTH', '_PHYS14D', '_MENT14D', '_HLTHPLN', '_HCVU652', '_TOTINDA', '_EXTETH3', '_DENVST3', '_MICHD', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR2', '_PRACE2', '_MRACE2', '_HISPANC', '_RACE1', '_RACEG22', '_RACEGR4', '_RACEPR1', '_SEX', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', 'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG1', '_SMOKER3', '_RFSMOK3', '_CURECI2', '_SMOKGRP', 'DRNKANY6', 'DROCDY4_', '_RFBING6', '_DRNKWK2', '_RFDRHV8', '_AIDTST4']\n",
      "\n",
      "üéØ BI·∫æN M·ª§C TI√äU TI·ªÄM NƒÇNG:\n",
      "   - LASTDEN4: 8 unique values, 1,363 missing\n",
      "   - RMVTETH4: 6 unique values, 1,363 missing\n",
      "   - _DENVST3: 3 unique values, 4 missing\n",
      "üíæ B·ªô nh·ªõ: 505.59 MB\n",
      "‚ùì Missing values: 963,258\n",
      "\n",
      "üìã C√ÅC C·ªòT TRONG DATASET:\n",
      "   ['_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU', 'SEXVAR', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'PRIMINSR', 'PERSDOC3', 'MEDCOST1', 'CHECKUP1', 'EXERANY2', 'SLEPTIM1', 'LASTDEN4', 'RMVTETH4', 'CVDINFR4', 'CVDCRHD4', 'CVDSTRK3', 'ASTHMA3', 'CHCSCNC1', 'CHCOCNC1', 'CHCCOPD3', 'ADDEPEV3', 'CHCKDNY2', 'HAVARTH4', 'DIABETE4', 'MARITAL', 'EDUCA', 'RENTHOM1', 'CPDEMO1C', 'VETERAN3', 'EMPLOY1', 'CHILDREN', 'INCOME3', 'WEIGHT2', 'HEIGHT3', 'DEAF', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'USENOW3', 'ECIGNOW2', 'LCSCTSC1', 'ALCDAY4', 'FLUSHOT7', 'PNEUVAC4', 'TETANUS1', 'HIVTST7', 'HIVRISK5', 'COVIDPOS', 'QSTVER', 'QSTLANG', '_METSTAT', '_URBSTAT', '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_IMPRACE', '_DUALUSE', '_LLCPWT2', '_LLCPWT', '_RFHLTH', '_PHYS14D', '_MENT14D', '_HLTHPLN', '_HCVU652', '_TOTINDA', '_EXTETH3', '_DENVST3', '_MICHD', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR2', '_PRACE2', '_MRACE2', '_HISPANC', '_RACE1', '_RACEG22', '_RACEGR4', '_RACEPR1', '_SEX', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', 'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG1', '_SMOKER3', '_RFSMOK3', '_CURECI2', '_SMOKGRP', 'DRNKANY6', 'DROCDY4_', '_RFBING6', '_DRNKWK2', '_RFDRHV8', '_AIDTST4']\n",
      "\n",
      "üéØ BI·∫æN M·ª§C TI√äU TI·ªÄM NƒÇNG:\n",
      "   - LASTDEN4: 8 unique values, 1,363 missing\n",
      "   - RMVTETH4: 6 unique values, 1,363 missing\n",
      "   - _DENVST3: 3 unique values, 4 missing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_STATE</th>\n",
       "      <th>FMONTH</th>\n",
       "      <th>IDATE</th>\n",
       "      <th>IMONTH</th>\n",
       "      <th>IDAY</th>\n",
       "      <th>IYEAR</th>\n",
       "      <th>DISPCODE</th>\n",
       "      <th>SEQNO</th>\n",
       "      <th>_PSU</th>\n",
       "      <th>SEXVAR</th>\n",
       "      <th>...</th>\n",
       "      <th>_SMOKER3</th>\n",
       "      <th>_RFSMOK3</th>\n",
       "      <th>_CURECI2</th>\n",
       "      <th>_SMOKGRP</th>\n",
       "      <th>DRNKANY6</th>\n",
       "      <th>DROCDY4_</th>\n",
       "      <th>_RFBING6</th>\n",
       "      <th>_DRNKWK2</th>\n",
       "      <th>_RFDRHV8</th>\n",
       "      <th>_AIDTST4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02032022</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>2022</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2022000001</td>\n",
       "      <td>2.022000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02042022</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>2022</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2022000002</td>\n",
       "      <td>2.022000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02022022</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2022000003</td>\n",
       "      <td>2.022000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02032022</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>2022</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2022000004</td>\n",
       "      <td>2.022000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>02022022</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>2022</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2022000005</td>\n",
       "      <td>2.022000e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _STATE  FMONTH     IDATE IMONTH IDAY IYEAR  DISPCODE       SEQNO  \\\n",
       "0     1.0     1.0  02032022     02   03  2022    1100.0  2022000001   \n",
       "1     1.0     1.0  02042022     02   04  2022    1100.0  2022000002   \n",
       "2     1.0     1.0  02022022     02   02  2022    1100.0  2022000003   \n",
       "3     1.0     1.0  02032022     02   03  2022    1100.0  2022000004   \n",
       "4     1.0     1.0  02022022     02   02  2022    1100.0  2022000005   \n",
       "\n",
       "           _PSU  SEXVAR  ...  _SMOKER3  _RFSMOK3  _CURECI2  _SMOKGRP  \\\n",
       "0  2.022000e+09     2.0  ...       4.0       1.0       1.0       4.0   \n",
       "1  2.022000e+09     2.0  ...       4.0       1.0       1.0       4.0   \n",
       "2  2.022000e+09     2.0  ...       4.0       1.0       1.0       4.0   \n",
       "3  2.022000e+09     2.0  ...       2.0       2.0       1.0       3.0   \n",
       "4  2.022000e+09     2.0  ...       4.0       1.0       1.0       4.0   \n",
       "\n",
       "   DRNKANY6  DROCDY4_  _RFBING6  _DRNKWK2  _RFDRHV8  _AIDTST4  \n",
       "0       2.0       0.0       1.0       0.0       1.0       2.0  \n",
       "1       2.0       0.0       1.0       0.0       1.0       2.0  \n",
       "2       2.0       0.0       1.0       0.0       1.0       2.0  \n",
       "3       2.0       0.0       1.0       0.0       1.0       2.0  \n",
       "4       1.0      10.0       1.0     140.0       1.0       2.0  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# T·∫£i dataset ƒë√£ clean t·ª´ EDA notebook\n",
    "print(\"üìä T·∫¢I DATASET ƒê√É CLEAN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # T·∫£i d·ªØ li·ªáu ƒë√£ clean\n",
    "    df = pd.read_parquet('../data/llcp2022_cleaned.parquet')\n",
    "    print(f\"‚úÖ T·∫£i th√†nh c√¥ng dataset ƒë√£ clean\")\n",
    "    print(f\"üìä K√≠ch th∆∞·ªõc: {df.shape[0]:,} h√†ng √ó {df.shape[1]} c·ªôt\")\n",
    "    print(f\"üíæ B·ªô nh·ªõ: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"‚ùì Missing values: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y file ƒë√£ clean!\")\n",
    "    print(\"Vui l√≤ng ch·∫°y EDA notebook tr∆∞·ªõc ƒë·ªÉ t·∫°o file cleaned data\")\n",
    "    \n",
    "# Hi·ªÉn th·ªã c√°c c·ªôt c√≥ s·∫µn\n",
    "print(f\"\\nüìã C√ÅC C·ªòT TRONG DATASET:\")\n",
    "print(f\"   {list(df.columns)}\")\n",
    "\n",
    "# T√¨m c√°c bi·∫øn m·ª•c ti√™u ti·ªÅm nƒÉng\n",
    "potential_targets = ['LASTDEN4', 'RMVTETH4', '_DENVST3']\n",
    "available_targets = [col for col in potential_targets if col in df.columns]\n",
    "\n",
    "print(f\"\\nüéØ BI·∫æN M·ª§C TI√äU TI·ªÄM NƒÇNG:\")\n",
    "for target in available_targets:\n",
    "    print(f\"   - {target}: {df[target].nunique()} unique values, {df[target].isnull().sum():,} missing\")\n",
    "\n",
    "# Hi·ªÉn th·ªã sample data\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02687e1",
   "metadata": {},
   "source": [
    "## 2. Ch·ªçn Target Variable v√† Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "696bfe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CH·ªåN TARGET VARIABLE\n",
      "========================================\n",
      "\n",
      "--- LASTDEN4 ---\n",
      "Missing: 0.3%\n",
      "Sample size: 443,769\n",
      "Unique values: 8\n",
      "Value distribution:\n",
      "   1.0: 292,408 (65.9%)\n",
      "   2.0: 50,326 (11.3%)\n",
      "   3.0: 46,987 (10.6%)\n",
      "   4.0: 44,828 (10.1%)\n",
      "   7.0: 4,866 (1.1%)\n",
      "   8.0: 3,562 (0.8%)\n",
      "   9.0: 788 (0.2%)\n",
      "   5.0: 4 (0.0%)\n",
      "\n",
      "--- RMVTETH4 ---\n",
      "Missing: 0.3%\n",
      "Sample size: 443,769\n",
      "Unique values: 6\n",
      "Value distribution:\n",
      "   8.0: 233,455 (52.6%)\n",
      "   1.0: 129,294 (29.1%)\n",
      "   2.0: 45,570 (10.3%)\n",
      "   3.0: 25,453 (5.7%)\n",
      "   7.0: 8,563 (1.9%)\n",
      "   9.0: 1,434 (0.3%)\n",
      "\n",
      "--- _DENVST3 ---\n",
      "Missing: 0.0%\n",
      "Sample size: 445,128\n",
      "Unique values: 3\n",
      "Value distribution:\n",
      "   1.0: 292,408 (65.7%)\n",
      "   2.0: 145,703 (32.7%)\n",
      "   9.0: 7,017 (1.6%)\n",
      "\n",
      "LASTDEN4 - Score: 8/9\n",
      "\n",
      "RMVTETH4 - Score: 8/9\n",
      "\n",
      "_DENVST3 - Score: 9/9\n",
      "\n",
      "üèÜ CH·ªåN TARGET VARIABLE: _DENVST3 (Score: 9/9)\n",
      "\n",
      "üìä TARGET VARIABLE: _DENVST3\n",
      "‚ö†Ô∏è  Target c√≥ 3 classes, ƒëang xem x√©t nh√≥m l·∫°i cho binary classification...\n",
      "\n",
      "üìä DATASET CU·ªêI C√ôNG:\n",
      "   - K√≠ch th∆∞·ªõc: 445,128 samples √ó 114 features\n",
      "   - Target distribution:\n",
      "     Class 1.0: 292,408 (65.7%)\n",
      "     Class 2.0: 145,703 (32.7%)\n",
      "     Class 9.0: 7,017 (1.6%)\n",
      "   - Minority class: 1.6%\n",
      "   - ‚ö†Ô∏è  Imbalanced dataset - c·∫ßn x·ª≠ l√Ω khi training\n",
      "\n",
      "üìä DATASET CU·ªêI C√ôNG:\n",
      "   - K√≠ch th∆∞·ªõc: 445,128 samples √ó 114 features\n",
      "   - Target distribution:\n",
      "     Class 1.0: 292,408 (65.7%)\n",
      "     Class 2.0: 145,703 (32.7%)\n",
      "     Class 9.0: 7,017 (1.6%)\n",
      "   - Minority class: 1.6%\n",
      "   - ‚ö†Ô∏è  Imbalanced dataset - c·∫ßn x·ª≠ l√Ω khi training\n"
     ]
    }
   ],
   "source": [
    "# Ch·ªçn target variable d·ª±a tr√™n ph√¢n t√≠ch EDA\n",
    "print(\"üéØ CH·ªåN TARGET VARIABLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Ph√¢n t√≠ch t·ª´ng target ti·ªÅm nƒÉng\n",
    "target_analysis = {}\n",
    "\n",
    "for target in available_targets:\n",
    "    data = df[target].dropna()\n",
    "    \n",
    "    target_analysis[target] = {\n",
    "        'missing_pct': df[target].isnull().mean() * 100,\n",
    "        'unique_values': data.nunique(),\n",
    "        'sample_size': len(data),\n",
    "        'value_counts': data.value_counts().head(10)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- {target} ---\")\n",
    "    print(f\"Missing: {target_analysis[target]['missing_pct']:.1f}%\")\n",
    "    print(f\"Sample size: {target_analysis[target]['sample_size']:,}\")\n",
    "    print(f\"Unique values: {target_analysis[target]['unique_values']}\")\n",
    "    print(f\"Value distribution:\")\n",
    "    for val, count in target_analysis[target]['value_counts'].items():\n",
    "        print(f\"   {val}: {count:,} ({count/target_analysis[target]['sample_size']*100:.1f}%)\")\n",
    "\n",
    "# T·ª± ƒë·ªông ch·ªçn target variable t·ªët nh·∫•t\n",
    "# Ti√™u ch√≠: missing < 15%, c√≥ 2-10 unique values, distribution c√¢n b·∫±ng\n",
    "best_target = None\n",
    "best_score = 0\n",
    "\n",
    "for target in available_targets:\n",
    "    score = 0\n",
    "    \n",
    "    # ƒêi·ªÉm cho missing rate\n",
    "    if target_analysis[target]['missing_pct'] < 15:\n",
    "        score += 3\n",
    "    elif target_analysis[target]['missing_pct'] < 25:\n",
    "        score += 2\n",
    "    else:\n",
    "        score += 1\n",
    "    \n",
    "    # ƒêi·ªÉm cho s·ªë unique values\n",
    "    if 2 <= target_analysis[target]['unique_values'] <= 5:\n",
    "        score += 3\n",
    "    elif target_analysis[target]['unique_values'] <= 10:\n",
    "        score += 2\n",
    "    else:\n",
    "        score += 1\n",
    "    \n",
    "    # ƒêi·ªÉm cho sample size\n",
    "    if target_analysis[target]['sample_size'] > 100000:\n",
    "        score += 3\n",
    "    elif target_analysis[target]['sample_size'] > 50000:\n",
    "        score += 2\n",
    "    else:\n",
    "        score += 1\n",
    "    \n",
    "    print(f\"\\n{target} - Score: {score}/9\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_target = target\n",
    "\n",
    "print(f\"\\nüèÜ CH·ªåN TARGET VARIABLE: {best_target} (Score: {best_score}/9)\")\n",
    "\n",
    "# Thi·∫øt l·∫≠p target variable\n",
    "TARGET_COLUMN = best_target\n",
    "print(f\"\\nüìä TARGET VARIABLE: {TARGET_COLUMN}\")\n",
    "\n",
    "# X·ª≠ l√Ω target variable cho classification\n",
    "y = df[TARGET_COLUMN].copy()\n",
    "\n",
    "# N·∫øu target c√≥ nhi·ªÅu h∆°n 2 classes, c√≥ th·ªÉ c·∫ßn nh√≥m l·∫°i\n",
    "if y.nunique() > 2:\n",
    "    print(f\"‚ö†Ô∏è  Target c√≥ {y.nunique()} classes, ƒëang xem x√©t nh√≥m l·∫°i cho binary classification...\")\n",
    "    \n",
    "    # V√≠ d·ª•: nh√≥m l·∫°i cho LASTDEN4 (l·∫ßn cu·ªëi ƒë·∫øn nha sƒ©)\n",
    "    if TARGET_COLUMN == 'LASTDEN4':\n",
    "        # 1: trong nƒÉm qua, 2: 1-2 nƒÉm, 3: 2-5 nƒÉm, 4: >5 nƒÉm, 8: ch∆∞a bao gi·ªù\n",
    "        # Nh√≥m th√†nh: 0 = kh√¥ng ƒë·ªÅu ƒë·∫∑n (2,3,4,8), 1 = ƒë·ªÅu ƒë·∫∑n (1)\n",
    "        y = (y == 1).astype(int)\n",
    "        print(f\"   ‚Üí Nh√≥m l·∫°i th√†nh: 0=kh√¥ng ƒë·ªÅu ƒë·∫∑n, 1=ƒë·ªÅu ƒë·∫∑n (ƒë·∫øn nha sƒ© h√†ng nƒÉm)\")\n",
    "    \n",
    "    elif TARGET_COLUMN == 'RMVTETH4':\n",
    "        # 1: 1-5 rƒÉng, 2: 6+ rƒÉng, 3: t·∫•t c·∫£, 8: kh√¥ng c√≥\n",
    "        # Nh√≥m th√†nh: 0 = kh√¥ng m·∫•t rƒÉng (8), 1 = c√≥ m·∫•t rƒÉng (1,2,3)\n",
    "        y = (y.isin([1, 2, 3])).astype(int)\n",
    "        print(f\"   ‚Üí Nh√≥m l·∫°i th√†nh: 0=kh√¥ng m·∫•t rƒÉng, 1=c√≥ m·∫•t rƒÉng do b·ªánh\")\n",
    "\n",
    "# Lo·∫°i b·ªè missing values trong target\n",
    "valid_indices = y.notna()\n",
    "y = y[valid_indices]\n",
    "X = df.drop(columns=[TARGET_COLUMN])[valid_indices]\n",
    "\n",
    "print(f\"\\nüìä DATASET CU·ªêI C√ôNG:\")\n",
    "print(f\"   - K√≠ch th∆∞·ªõc: {X.shape[0]:,} samples √ó {X.shape[1]} features\")\n",
    "print(f\"   - Target distribution:\")\n",
    "for val, count in y.value_counts().items():\n",
    "    print(f\"     Class {val}: {count:,} ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "# Ki·ªÉm tra class imbalance\n",
    "minority_class_pct = y.value_counts().min() / len(y) * 100\n",
    "print(f\"   - Minority class: {minority_class_pct:.1f}%\")\n",
    "if minority_class_pct < 20:\n",
    "    print(f\"   - ‚ö†Ô∏è  Imbalanced dataset - c·∫ßn x·ª≠ l√Ω khi training\")\n",
    "else:\n",
    "    print(f\"   - ‚úÖ Balanced dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703917df",
   "metadata": {},
   "source": [
    "## 3. Chia Dataset v√† Chu·∫©n b·ªã Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240e08ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ CHIA DATASET V√Ä CHU·∫®N B·ªä FEATURES\n",
      "==================================================\n",
      "üìä K√çCH TH∆Ø·ªöC DATASETS:\n",
      "   - Train: 267,076 samples (60.0%)\n",
      "   - Validation: 89,026 samples (20.0%)\n",
      "   - Test: 89,026 samples (20.0%)\n",
      "\n",
      "üìà DISTRIBUTION TRONG T·ª™NG SET:\n",
      "   - Train Distribution:\n",
      "     ‚Ä¢ Class 1.0: 65.7%\n",
      "     ‚Ä¢ Class 2.0: 32.7%\n",
      "     ‚Ä¢ Class 9.0: 1.6%\n",
      "   - Validation Distribution:\n",
      "     ‚Ä¢ Class 1.0: 65.7%\n",
      "     ‚Ä¢ Class 2.0: 32.7%\n",
      "     ‚Ä¢ Class 9.0: 1.6%\n",
      "   - Test Distribution:\n",
      "     ‚Ä¢ Class 1.0: 65.7%\n",
      "     ‚Ä¢ Class 2.0: 32.7%\n",
      "     ‚Ä¢ Class 9.0: 1.6%\n",
      "\n",
      "üîß X·ª¨ L√ù FEATURES:\n",
      "   - Missing values in train: 579121\n",
      "üìä K√çCH TH∆Ø·ªöC DATASETS:\n",
      "   - Train: 267,076 samples (60.0%)\n",
      "   - Validation: 89,026 samples (20.0%)\n",
      "   - Test: 89,026 samples (20.0%)\n",
      "\n",
      "üìà DISTRIBUTION TRONG T·ª™NG SET:\n",
      "   - Train Distribution:\n",
      "     ‚Ä¢ Class 1.0: 65.7%\n",
      "     ‚Ä¢ Class 2.0: 32.7%\n",
      "     ‚Ä¢ Class 9.0: 1.6%\n",
      "   - Validation Distribution:\n",
      "     ‚Ä¢ Class 1.0: 65.7%\n",
      "     ‚Ä¢ Class 2.0: 32.7%\n",
      "     ‚Ä¢ Class 9.0: 1.6%\n",
      "   - Test Distribution:\n",
      "     ‚Ä¢ Class 1.0: 65.7%\n",
      "     ‚Ä¢ Class 2.0: 32.7%\n",
      "     ‚Ä¢ Class 9.0: 1.6%\n",
      "\n",
      "üîß X·ª¨ L√ù FEATURES:\n",
      "   - Missing values in train: 579121\n",
      "   - Columns with missing values: 67\n",
      "   - Numeric features: 109\n",
      "   - Categorical features: 5\n",
      "   - üîÑ Imputing missing values...\n",
      "   - Columns with missing values: 67\n",
      "   - Numeric features: 109\n",
      "   - Categorical features: 5\n",
      "   - üîÑ Imputing missing values...\n",
      "   - ‚úÖ Imputed numeric features with median\n",
      "   - ‚úÖ Imputed categorical features with mode\n",
      "   - ‚úÖ Imputed numeric features with median\n",
      "   - ‚úÖ Imputed categorical features with mode\n",
      "   - Missing values after imputation: 0\n",
      "   - Missing values after imputation: 0\n",
      "   - ‚úÖ Standardized numeric features\n",
      "   - ‚úÖ Standardized numeric features\n",
      "   - ‚úÖ Encoded categorical features\n",
      "\n",
      "‚úÖ CHU·∫®N B·ªä FEATURES HO√ÄN TH√ÄNH!\n",
      "   - Features shape: (267076, 114)\n",
      "   - All features are numeric: True\n",
      "   - No missing values in train: True\n",
      "   - No missing values in val: True\n",
      "   - No missing values in test: True\n",
      "   - No NaN in numpy arrays: True\n",
      "\n",
      "üéØ TARGET VARIABLE INFO:\n",
      "   - Target column: _DENVST3\n",
      "   - Unique classes: [np.float64(1.0), np.float64(2.0), np.float64(9.0)]\n",
      "   - Number of classes: 3\n",
      "   - Most frequent class: 1.0 (65.7%)\n",
      "   - Least frequent class: 9.0 (1.6%)\n",
      "   - ‚úÖ Encoded categorical features\n",
      "\n",
      "‚úÖ CHU·∫®N B·ªä FEATURES HO√ÄN TH√ÄNH!\n",
      "   - Features shape: (267076, 114)\n",
      "   - All features are numeric: True\n",
      "   - No missing values in train: True\n",
      "   - No missing values in val: True\n",
      "   - No missing values in test: True\n",
      "   - No NaN in numpy arrays: True\n",
      "\n",
      "üéØ TARGET VARIABLE INFO:\n",
      "   - Target column: _DENVST3\n",
      "   - Unique classes: [np.float64(1.0), np.float64(2.0), np.float64(9.0)]\n",
      "   - Number of classes: 3\n",
      "   - Most frequent class: 1.0 (65.7%)\n",
      "   - Least frequent class: 9.0 (1.6%)\n"
     ]
    }
   ],
   "source": [
    "# Chia dataset th√†nh train/validation/test sets\n",
    "print(\"üîÑ CHIA DATASET V√Ä CHU·∫®N B·ªä FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Chia train (60%) / validation (20%) / test (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìä K√çCH TH∆Ø·ªöC DATASETS:\")\n",
    "print(f\"   - Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Ki·ªÉm tra distribution trong t·ª´ng set\n",
    "print(f\"\\nüìà DISTRIBUTION TRONG T·ª™NG SET:\")\n",
    "for name, y_set in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    dist = y_set.value_counts(normalize=True) * 100\n",
    "    print(f\"   - {name} Distribution:\")\n",
    "    for class_val, pct in dist.items():\n",
    "        print(f\"     ‚Ä¢ Class {class_val}: {pct:.1f}%\")\n",
    "\n",
    "# X·ª≠ l√Ω features\n",
    "print(f\"\\nüîß X·ª¨ L√ù FEATURES:\")\n",
    "\n",
    "# Ki·ªÉm tra missing values\n",
    "print(f\"   - Missing values in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"   - Columns with missing values: {X_train.isnull().sum()[X_train.isnull().sum() > 0].shape[0]}\")\n",
    "\n",
    "# T√°ch numeric v√† categorical features\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"   - Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   - Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Imputation for missing values\n",
    "print(f\"   - üîÑ Imputing missing values...\")\n",
    "\n",
    "# Copy data for processing\n",
    "X_train_processed = X_train.copy()\n",
    "X_val_processed = X_val.copy()\n",
    "X_test_processed = X_test.copy()\n",
    "\n",
    "# Impute numeric features with median\n",
    "if numeric_features:\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    X_train_processed[numeric_features] = numeric_imputer.fit_transform(X_train_processed[numeric_features])\n",
    "    X_val_processed[numeric_features] = numeric_imputer.transform(X_val_processed[numeric_features])\n",
    "    X_test_processed[numeric_features] = numeric_imputer.transform(X_test_processed[numeric_features])\n",
    "    print(f\"   - ‚úÖ Imputed numeric features with median\")\n",
    "\n",
    "# Impute categorical features with mode\n",
    "if categorical_features:\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_processed[categorical_features] = categorical_imputer.fit_transform(X_train_processed[categorical_features])\n",
    "    X_val_processed[categorical_features] = categorical_imputer.transform(X_val_processed[categorical_features])\n",
    "    X_test_processed[categorical_features] = categorical_imputer.transform(X_test_processed[categorical_features])\n",
    "    print(f\"   - ‚úÖ Imputed categorical features with mode\")\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"   - Missing values after imputation: {X_train_processed.isnull().sum().sum()}\")\n",
    "\n",
    "# Standardize numeric features\n",
    "if numeric_features:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train_processed.copy()\n",
    "    X_val_scaled = X_val_processed.copy()\n",
    "    X_test_scaled = X_test_processed.copy()\n",
    "    \n",
    "    X_train_scaled[numeric_features] = scaler.fit_transform(X_train_processed[numeric_features])\n",
    "    X_val_scaled[numeric_features] = scaler.transform(X_val_processed[numeric_features])\n",
    "    X_test_scaled[numeric_features] = scaler.transform(X_test_processed[numeric_features])\n",
    "    \n",
    "    print(f\"   - ‚úÖ Standardized numeric features\")\n",
    "else:\n",
    "    X_train_scaled = X_train_processed.copy()\n",
    "    X_val_scaled = X_val_processed.copy()\n",
    "    X_test_scaled = X_test_processed.copy()\n",
    "    scaler = None\n",
    "\n",
    "# Encode categorical features\n",
    "if categorical_features:\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Fit tr√™n train data\n",
    "        X_train_scaled[col] = le.fit_transform(X_train_scaled[col].astype(str))\n",
    "        \n",
    "        # Transform validation v√† test data\n",
    "        # X·ª≠ l√Ω unseen categories\n",
    "        def safe_transform(data, encoder):\n",
    "            known_classes = set(encoder.classes_)\n",
    "            data_str = data.astype(str)\n",
    "            \n",
    "            # Thay th·∫ø unknown categories b·∫±ng most frequent class\n",
    "            most_frequent = encoder.classes_[0]  # ho·∫∑c c√≥ th·ªÉ d√πng mode\n",
    "            data_str = data_str.apply(lambda x: x if x in known_classes else most_frequent)\n",
    "            \n",
    "            return encoder.transform(data_str)\n",
    "        \n",
    "        X_val_scaled[col] = safe_transform(X_val_scaled[col], le)\n",
    "        X_test_scaled[col] = safe_transform(X_test_scaled[col], le)\n",
    "        \n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    print(f\"   - ‚úÖ Encoded categorical features\")\n",
    "else:\n",
    "    label_encoders = {}\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi th√†nh numpy arrays cho m·ªôt s·ªë models\n",
    "X_train_np = X_train_scaled.values\n",
    "X_val_np = X_val_scaled.values\n",
    "X_test_np = X_test_scaled.values\n",
    "\n",
    "y_train_np = y_train.values\n",
    "y_val_np = y_val.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "print(f\"\\n‚úÖ CHU·∫®N B·ªä FEATURES HO√ÄN TH√ÄNH!\")\n",
    "print(f\"   - Features shape: {X_train_np.shape}\")\n",
    "print(f\"   - All features are numeric: {np.all([np.issubdtype(X_train_scaled[col].dtype, np.number) for col in X_train_scaled.columns])}\")\n",
    "print(f\"   - No missing values in train: {not X_train_scaled.isnull().any().any()}\")\n",
    "print(f\"   - No missing values in val: {not X_val_scaled.isnull().any().any()}\")\n",
    "print(f\"   - No missing values in test: {not X_test_scaled.isnull().any().any()}\")\n",
    "print(f\"   - No NaN in numpy arrays: {not np.isnan(X_train_np).any()}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin v·ªÅ target variable\n",
    "print(f\"\\nüéØ TARGET VARIABLE INFO:\")\n",
    "print(f\"   - Target column: {TARGET_COLUMN}\")\n",
    "print(f\"   - Unique classes: {sorted(y.unique())}\")\n",
    "print(f\"   - Number of classes: {y.nunique()}\")\n",
    "print(f\"   - Most frequent class: {y.mode().iloc[0]} ({(y.value_counts().iloc[0] / len(y) * 100):.1f}%)\")\n",
    "print(f\"   - Least frequent class: {y.value_counts().index[-1]} ({(y.value_counts().iloc[-1] / len(y) * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0d578",
   "metadata": {},
   "source": [
    "## 4. Training 10 Baseline Models\n",
    "\n",
    "Theo k·∫ø ho·∫°ch team, ch√∫ng ta s·∫Ω train 10 models:\n",
    "- **Classical**: SVM, KNN\n",
    "- **Boosting**: XGBoost, CatBoost, AdaBoost\n",
    "- **Neural Networks**: MLP, 1D CNN, TabNet\n",
    "- **Additional**: Random Forest, Logistic Regression, Naive Bayes\n",
    "\n",
    "**Focus**: T·ªëi ∆∞u h√≥a **Recall** cho b√†i to√°n screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a806697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ TRAINING 10 BASELINE MODELS\n",
      "==================================================\n",
      "\n",
      "üîπ CLASSICAL MODELS\n",
      "------------------------------\n",
      "Training SVM...\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªãnh nghƒ©a v√† train 10 baseline models\n",
    "print(\"ü§ñ TRAINING 10 BASELINE MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dictionary ƒë·ªÉ l∆∞u models v√† k·∫øt qu·∫£\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# H√†m ƒë√°nh gi√° model v·ªõi focus on Recall\n",
    "def evaluate_model(model, X_val, y_val, model_name):\n",
    "    \"\"\"ƒê√°nh gi√° model v·ªõi focus on Recall cho screening\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "    \n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Import additional models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 1. Classical Models\n",
    "print(\"\\nüîπ CLASSICAL MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# SVM\n",
    "print(\"Training SVM...\")\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_np, y_train_np)\n",
    "models['SVM'] = svm_model\n",
    "results['SVM'] = evaluate_model(svm_model, X_val_np, y_val_np, 'SVM')\n",
    "print(f\"  SVM - Recall: {results['SVM']['recall']:.3f}, Precision: {results['SVM']['precision']:.3f}\")\n",
    "\n",
    "# KNN\n",
    "print(\"Training KNN...\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_np, y_train_np)\n",
    "models['KNN'] = knn_model\n",
    "results['KNN'] = evaluate_model(knn_model, X_val_np, y_val_np, 'KNN')\n",
    "print(f\"  KNN - Recall: {results['KNN']['recall']:.3f}, Precision: {results['KNN']['precision']:.3f}\")\n",
    "\n",
    "# 2. Boosting Models\n",
    "print(\"\\nüîπ BOOSTING MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_np, y_train_np)\n",
    "models['XGBoost'] = xgb_model\n",
    "results['XGBoost'] = evaluate_model(xgb_model, X_val_np, y_val_np, 'XGBoost')\n",
    "print(f\"  XGBoost - Recall: {results['XGBoost']['recall']:.3f}, Precision: {results['XGBoost']['precision']:.3f}\")\n",
    "\n",
    "# CatBoost\n",
    "print(\"Training CatBoost...\")\n",
    "catboost_model = CatBoostClassifier(random_state=42, silent=True)\n",
    "catboost_model.fit(X_train_np, y_train_np)\n",
    "models['CatBoost'] = catboost_model\n",
    "results['CatBoost'] = evaluate_model(catboost_model, X_val_np, y_val_np, 'CatBoost')\n",
    "print(f\"  CatBoost - Recall: {results['CatBoost']['recall']:.3f}, Precision: {results['CatBoost']['precision']:.3f}\")\n",
    "\n",
    "# AdaBoost\n",
    "print(\"Training AdaBoost...\")\n",
    "ada_model = AdaBoostClassifier(random_state=42)\n",
    "ada_model.fit(X_train_np, y_train_np)\n",
    "models['AdaBoost'] = ada_model\n",
    "results['AdaBoost'] = evaluate_model(ada_model, X_val_np, y_val_np, 'AdaBoost')\n",
    "print(f\"  AdaBoost - Recall: {results['AdaBoost']['recall']:.3f}, Precision: {results['AdaBoost']['precision']:.3f}\")\n",
    "\n",
    "# 3. Neural Networks\n",
    "print(\"\\nüîπ NEURAL NETWORKS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# MLP\n",
    "print(\"Training MLP...\")\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)\n",
    "mlp_model.fit(X_train_np, y_train_np)\n",
    "models['MLP'] = mlp_model\n",
    "results['MLP'] = evaluate_model(mlp_model, X_val_np, y_val_np, 'MLP')\n",
    "print(f\"  MLP - Recall: {results['MLP']['recall']:.3f}, Precision: {results['MLP']['precision']:.3f}\")\n",
    "\n",
    "# 4. Additional Models\n",
    "print(\"\\nüîπ ADDITIONAL MODELS\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_np, y_train_np)\n",
    "models['RandomForest'] = rf_model\n",
    "results['RandomForest'] = evaluate_model(rf_model, X_val_np, y_val_np, 'RandomForest')\n",
    "print(f\"  RandomForest - Recall: {results['RandomForest']['recall']:.3f}, Precision: {results['RandomForest']['precision']:.3f}\")\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_np, y_train_np)\n",
    "models['LogisticRegression'] = lr_model\n",
    "results['LogisticRegression'] = evaluate_model(lr_model, X_val_np, y_val_np, 'LogisticRegression')\n",
    "print(f\"  LogisticRegression - Recall: {results['LogisticRegression']['recall']:.3f}, Precision: {results['LogisticRegression']['precision']:.3f}\")\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"Training Naive Bayes...\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_np, y_train_np)\n",
    "models['NaiveBayes'] = nb_model\n",
    "results['NaiveBayes'] = evaluate_model(nb_model, X_val_np, y_val_np, 'NaiveBayes')\n",
    "print(f\"  NaiveBayes - Recall: {results['NaiveBayes']['recall']:.3f}, Precision: {results['NaiveBayes']['precision']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ HO√ÄN TH√ÄNH TRAINING {len(models)} MODELS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bac7f1",
   "metadata": {},
   "source": [
    "## 5. So s√°nh Performance c·ªßa c√°c Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8110304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh performance c·ªßa t·∫•t c·∫£ models\n",
    "print(\"üìä SO S√ÅNH PERFORMANCE C·ª¶A C√ÅC MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# T·∫°o DataFrame ƒë·ªÉ so s√°nh\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [result['model_name'] for result in results.values()],\n",
    "    'Recall': [result['recall'] for result in results.values()],\n",
    "    'Precision': [result['precision'] for result in results.values()],\n",
    "    'F1': [result['f1'] for result in results.values()],\n",
    "    'Accuracy': [result['accuracy'] for result in results.values()],\n",
    "    'ROC_AUC': [result['roc_auc'] for result in results.values()]\n",
    "})\n",
    "\n",
    "# S·∫Øp x·∫øp theo Recall (quan tr·ªçng nh·∫•t cho screening)\n",
    "comparison_df = comparison_df.sort_values('Recall', ascending=False)\n",
    "\n",
    "print(\"üèÜ RANKING THEO RECALL (QUAN TR·ªåNG NH·∫§T CHO SCREENING):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Rank':<4} {'Model':<15} {'Recall':<8} {'Precision':<9} {'F1':<8} {'Accuracy':<8} {'ROC_AUC':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (_, row) in enumerate(comparison_df.iterrows(), 1):\n",
    "    roc_auc_str = f\"{row['ROC_AUC']:.3f}\" if row['ROC_AUC'] is not None else \"N/A\"\n",
    "    print(f\"{i:<4} {row['Model']:<15} {row['Recall']:<8.3f} {row['Precision']:<9.3f} {row['F1']:<8.3f} {row['Accuracy']:<8.3f} {roc_auc_str:<8}\")\n",
    "\n",
    "# T√¨m top 3 models theo Recall\n",
    "top_3_models = comparison_df.head(3)['Model'].tolist()\n",
    "print(f\"\\nü•á TOP 3 MODELS THEO RECALL:\")\n",
    "for i, model in enumerate(top_3_models, 1):\n",
    "    recall = comparison_df[comparison_df['Model'] == model]['Recall'].iloc[0]\n",
    "    print(f\"   {i}. {model}: {recall:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Recall comparison\n",
    "axes[0, 0].bar(comparison_df['Model'], comparison_df['Recall'], color='coral')\n",
    "axes[0, 0].set_title('Recall Comparison (Higher is Better for Screening)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Recall')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Precision vs Recall scatter\n",
    "axes[0, 1].scatter(comparison_df['Precision'], comparison_df['Recall'], s=100, alpha=0.7)\n",
    "for i, model in enumerate(comparison_df['Model']):\n",
    "    axes[0, 1].annotate(model, (comparison_df.iloc[i]['Precision'], comparison_df.iloc[i]['Recall']), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0, 1].set_xlabel('Precision')\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].set_title('Precision vs Recall Trade-off', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ROC AUC comparison\n",
    "roc_data = comparison_df[comparison_df['ROC_AUC'].notna()]\n",
    "axes[1, 0].bar(roc_data['Model'], roc_data['ROC_AUC'], color='lightblue')\n",
    "axes[1, 0].set_title('ROC AUC Comparison', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('ROC AUC')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. All metrics radar chart (ch·ªâ top 3)\n",
    "from math import pi\n",
    "\n",
    "metrics = ['Recall', 'Precision', 'F1', 'Accuracy']\n",
    "N = len(metrics)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.set_theta_offset(pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_thetagrids(range(0, 360, 360 // N), metrics)\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, model in enumerate(top_3_models[:3]):\n",
    "    values = [comparison_df[comparison_df['Model'] == model][metric].iloc[0] for metric in metrics]\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Top 3 Models - All Metrics', fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "comparison_df.to_csv('../results/model_comparison.csv', index=False)\n",
    "print(f\"\\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ so s√°nh: ../results/model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DENTAL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
